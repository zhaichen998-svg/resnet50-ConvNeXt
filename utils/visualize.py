"""
Visualization utilities for training results and model comparison
å¯è§†åŒ–å·¥å…· - ç»˜åˆ¶è®­ç»ƒæ›²çº¿ã€æ··æ·†çŸ©é˜µå’Œæ¨¡å‹å¯¹æ¯”å›¾
"""

import argparse
import json
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import seaborn as sns


def plot_results(results_dir, save_path=None):
    """
    ç»˜åˆ¶è®­ç»ƒç»“æœæ›²çº¿
    
    Args:
        results_dir: è®­ç»ƒç»“æœç›®å½•ï¼ˆåŒ…å« results.csvï¼‰
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    results_file = Path(results_dir) / 'results.csv'
    
    if not results_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶: {results_file}")
        return
    
    # è¯»å–ç»“æœæ•°æ®
    import pandas as pd
    df = pd.read_csv(results_file)
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Training Results', fontsize=16, fontweight='bold')
    
    # 1. Loss æ›²çº¿
    ax = axes[0, 0]
    if 'train/box_loss' in df.columns:
        ax.plot(df['epoch'], df['train/box_loss'], label='Box Loss', linewidth=2)
    if 'train/cls_loss' in df.columns:
        ax.plot(df['epoch'], df['train/cls_loss'], label='Cls Loss', linewidth=2)
    if 'train/dfl_loss' in df.columns:
        ax.plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss', linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('Training Loss')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. mAP æ›²çº¿
    ax = axes[0, 1]
    if 'metrics/mAP50(B)' in df.columns:
        ax.plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP@0.5', linewidth=2, color='green')
    if 'metrics/mAP50-95(B)' in df.columns:
        ax.plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', linewidth=2, color='blue')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('mAP')
    ax.set_title('Validation mAP')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. Precision & Recall
    ax = axes[1, 0]
    if 'metrics/precision(B)' in df.columns:
        ax.plot(df['epoch'], df['metrics/precision(B)'], label='Precision', linewidth=2, color='orange')
    if 'metrics/recall(B)' in df.columns:
        ax.plot(df['epoch'], df['metrics/recall(B)'], label='Recall', linewidth=2, color='purple')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Score')
    ax.set_title('Precision & Recall')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 4. Learning Rate
    ax = axes[1, 1]
    if 'lr/pg0' in df.columns:
        ax.plot(df['epoch'], df['lr/pg0'], label='LR', linewidth=2, color='red')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Learning Rate')
    ax.set_title('Learning Rate Schedule')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… å›¾è¡¨å·²ä¿å­˜: {save_path}")
    else:
        plt.show()


def plot_confusion_matrix(confusion_matrix, class_names, save_path=None):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    
    Args:
        confusion_matrix: æ··æ·†çŸ©é˜µæ•°ç»„
        class_names: ç±»åˆ«åç§°åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    plt.figure(figsize=(10, 8))
    
    # å½’ä¸€åŒ–
    cm_normalized = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis]
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Probability'})
    
    plt.title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    else:
        plt.show()


def compare_models(model_results, save_path=None):
    """
    å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
    
    Args:
        model_results: å­—å…¸ï¼Œæ ¼å¼ {æ¨¡å‹åç§°: {æŒ‡æ ‡å: å€¼}}
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Model Comparison', fontsize=16, fontweight='bold')
    
    models = list(model_results.keys())
    metrics = ['mAP50', 'mAP50-95', 'Precision', 'Recall']
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
    
    for idx, metric in enumerate(metrics):
        ax = axes[idx // 2, idx % 2]
        
        values = [model_results[model].get(metric, 0) for model in models]
        bars = ax.bar(models, values, color=colors[idx], alpha=0.8, edgecolor='black', linewidth=1.5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}',
                   ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        ax.set_ylabel('Score', fontsize=12)
        ax.set_title(metric, fontsize=14, fontweight='bold')
        ax.set_ylim([0, 1.0])
        ax.grid(True, alpha=0.3, axis='y')
        ax.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    else:
        plt.show()


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Visualization utilities')
    
    parser.add_argument('--mode', type=str, required=True,
                        choices=['results', 'compare'],
                        help='å¯è§†åŒ–æ¨¡å¼')
    parser.add_argument('--results_dir', type=str,
                        help='è®­ç»ƒç»“æœç›®å½•')
    parser.add_argument('--compare_json', type=str,
                        help='æ¨¡å‹å¯¹æ¯” JSON æ–‡ä»¶')
    parser.add_argument('--save', type=str,
                        help='ä¿å­˜è·¯å¾„')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    if args.mode == 'results':
        if not args.results_dir:
            print("âŒ é”™è¯¯: results æ¨¡å¼éœ€è¦æŒ‡å®š --results_dir")
            return
        
        print(f"ğŸ“Š ç»˜åˆ¶è®­ç»ƒç»“æœ: {args.results_dir}")
        plot_results(args.results_dir, args.save)
    
    elif args.mode == 'compare':
        if not args.compare_json:
            print("âŒ é”™è¯¯: compare æ¨¡å¼éœ€è¦æŒ‡å®š --compare_json")
            return
        
        # è¯»å–å¯¹æ¯”æ•°æ®
        with open(args.compare_json, 'r') as f:
            model_results = json.load(f)
        
        print(f"ğŸ“Š ç»˜åˆ¶æ¨¡å‹å¯¹æ¯”")
        compare_models(model_results, args.save)


if __name__ == '__main__':
    main()
